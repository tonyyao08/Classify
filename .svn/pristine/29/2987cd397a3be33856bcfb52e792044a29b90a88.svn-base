# written in top-down design approach
import boto3
import json
from PIL import Image
from os.path import exists
import requests


# hard-coded fields created from aws
bucket_name = "mycoins"
api_key = "da2-tkzuvhzfe5etrmisvfchmr7sia" 
url = 'https://ztgryu55q5hs3o6tff5lh3qovq.appsync-api.us-east-1.amazonaws.com/graphql'


# hard coded query for our projects items
query_project_items = """query {
listProjects {
    items {
    createdAt
    id
    imageBucket
    isTraining
    labels
    owner
    projectName
    updatedAt
    }
}
}"""


# helper function to run inital queries to communicate with web app
def run_query(query, api_key, url):
    data = {"query": query}
    json_data = json.dumps(data)
    header = {"X-API-Key": api_key}

    response = requests.post(url=url, headers=header, data=json_data)
    return json.loads(response.text)


project_data = run_query(query_project_items, api_key, url)
project_items = project_data.get("data").get("listProjects").get("items")[0]
# project name retrieved from amplify
project_name = project_items.get("projectName")
# project id needed to get current labels
project_id = project_items.get("id")

# intialize clients to communicate with aws services
s3 = boto3.client('s3')
rekognition = boto3.client("rekognition")

# local databases unique to each project
label_database = project_name + "-label.json"
bucket_database = project_name + "-bucket.json"


# run train mode if pi passes in true, otherwise run classify mode
def handler(image_path): 
    # if database doesn't exist, create databases
    if exists(bucket_database) == False:
        initalize_databases(label_database, bucket_database)
    train_handler()
    classify_handler(image_path)


# on startup, create databases if needed for new projects
def initalize_databases(label_database, bucket_database):
    empty_json = []
    with open(label_database, 'w') as f:
        json.dump(empty_json, f, indent=4)

    with open(bucket_database, 'w') as f:
        json.dump(empty_json, f, indent=4)


# update our local labels database from webapp prior to classification
def train_handler():
    # use project id to get these bucket items from below query
    bucket_items = """\n    items {
        label
        rekognitionMeta
        }
    }
    }
    """
    query_bucket_items =  ("query {\n" +
      "  listImages(filter: {userGenerated: {eq: true}, projectID: {eq: \"" + 
       project_id + "\"}}) { ") + bucket_items

    bucket_data = run_query(query_bucket_items, api_key, url)
    user_labels = bucket_data.get("data").get("listImages")
    with open(label_database, 'w') as f:     
        json.dump(user_labels, f, indent=4)

# detect text from images, try matching to labels and place images in a bucket
def classify_handler(image_path):
    # get image name from full image path given
    image_name = image_path.rsplit('/', 1)[1]
    flipped_image_name = image_name.split('.')[0] + "_flipped.jpg"

    # detect text for both images, merge them into all detected text
    original_detected_text = detect_text(image_name, bucket_name)
    print(original_detected_text)
    flipped_detected_text = detect_text(flipped_image_name, bucket_name)
    print(flipped_detected_text)
    all_detected_text = original_detected_text + flipped_detected_text
    print(all_detected_text)
    
    # check if detected text matches an existing label
    try:
        map_image_to_label(all_detected_text, image_name, label_database, bucket_database)
        # need to upload current image with label back to webapp now
        # use pi code for uploading image, add label param after image param
    except Exception as e:
        print(e)


# Utility functions for performming ML/database operations below


# upload an image to our s3 bucket
def upload_to_s3(image_path, image_name):
    try:
        s3.upload_file(image_path, bucket_name, image_name)
    except Exception as e:
        print(e)


# detect text from a given image
def detect_text(photo, s3_bucket):
    response = rekognition.detect_text(Image = {"S3Object": {"Bucket": s3_bucket, "Name": photo}})
    textDetections = response["TextDetections"]
    text_arr = []
    for text in textDetections:
        # only include text with over 75% confidence level
        if (text["Confidence"]) > 75:
            text_arr.append(text["DetectedText"])
    
    return text_arr


# check if an images detected text can be mapped to a label/bucket
def map_image_to_label(detect_text_response, image_name, label_database, bucket_database):
    with open(label_database) as lf:
        labels_list = json.load(lf)

    with open(bucket_database) as pf:
        prev_database = json.load(pf)

    label_name = "Unknown"
    labels = [d['label'] for d in labels_list]
    for entry in labels:
        user_text_arr = entry.get("matchingText")
        for user_text in user_text_arr:
            for detected_text in detect_text_response:
                # call our own function used for matching similar text
                if match_text(user_text.upper(), detected_text):
                    label_name = entry.get("name")

    new_bucket = {image_name:label_name}
    prev_database.append(new_bucket)
    # update local database with matching image name and label
    with open(bucket_database, 'w', encoding='utf-8') as f:
        json.dump(prev_database, f, indent=4)
    # update webapp with image and new label
    create_image_query = gen_web_image_query(image_name, project_id, label_name, False)
    data = {"query": create_image_query}
    json_data = json.dumps(data)
    header = {'X-API-Key': api_key}
    requests.post(url=url, headers=header, data=json_data)



# match similar words to allow for one off errors based off word length
def match_text(s1, s2):
    if s1 == s2:
        return True
    else:
        # convert words to char array to check for one off errors
        char_array1 = [char for char in s1]
        char_array2 = [char for char in s2]
        # get smallest and biggest length
        l1 = len(char_array1)
        l2 = len(char_array2)
        min_length = l1 if l1 < l2 else l2
        max_length = l1 if l1 > l2 else l2
        # initalize number of mismatched chars to difference between lengths 
        mismatch_count = max_length - min_length
        # accept one or more errors for words with > 4 characters
        if min_length < 5:
            return False
        return close_enough(char_array1, char_array2, min_length, mismatch_count)

# check if 2 words are close enough to be counted as the same
def close_enough(c1, c2, min_length, mismatch_count):
    # count occurences of mismatched characters
    for i in range(min_length):
        if c1[i] != c2[i]:
            mismatch_count+=1
    # accept 1 mismatch for words with length > 4 and < 8
    if min_length < 8:
        return True if mismatch_count <= 1 else False
    else: # accept 2 mismatches for words with length >= 8
        return True if mismatch_count <= 2 else False


def gen_web_image_query(image_name, proj_id, label_name, user_generated):
    final_image = image_name + ".jpg"
    create_image_query = f"""mutation {{ 
            createImage(input: {{imageKey: "{final_image}", projectID: "{proj_id}", label: "{label_name}", userGenerated: "{user_generated}"}}) {{ 
                id 
                _version
                createdAt
                updatedAt
                _lastChangedAt
                projectID
                _deleted
                imageKey
            }}
            }}"""  # .format(final_image = final_image, projID = projID)
    return create_image_query

